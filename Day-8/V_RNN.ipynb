{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad5058e-0f44-4cc8-9d8e-4f883ed6da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87789dea-53eb-43a6-8a14-e66d38d40a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample data consisting of English Alphabets \n",
    "inputs = np.array([\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "])\n",
    "\n",
    "expected = np.array([\n",
    "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41e65225-4b29-45cc-9250-25b11cc40941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to convert the list of strings into a list of one hot encoded vectors\n",
    "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
    "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
    "\n",
    "    one_hot_inputs = []\n",
    "    for row in inputs:\n",
    "        one_hot_list = []\n",
    "        for char in row:\n",
    "            if char.upper() in char_to_index:\n",
    "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
    "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
    "                one_hot_list.append(one_hot_vector)\n",
    "        one_hot_inputs.append(one_hot_list)\n",
    "\n",
    "    return np.array(one_hot_inputs)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab602de-90d2-47da-90a7-b64a6db0cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each input sequence will have the input shape of (26, 26, 1)\n",
    "class InputLayer:\n",
    "    inputs: np.ndarray # sequential data in the form of np arrays\n",
    "    U: np.ndarray = None # weight matrix connecting input to hidden layer\n",
    "    delta_U: np.ndarray = None # gradient calculated during BPTT\n",
    "\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.U = np.random.uniform(low=0, high=1, size=(hidden_size, len(inputs[0])))\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "\n",
    "    # Returns the one-hot encoded vector of the character at a give time step\n",
    "    def get_input(self, time_step: int) -> np.ndarray:\n",
    "        return self.inputs[time_step]\n",
    "\n",
    "    # Return the result of U . x[t] to be used in s_t\n",
    "    def weighted_sum(self, time_step: int) -> np.ndarray:\n",
    "        return self.U @ self.get_input(time_step)\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_weighted_sum: np.ndarray\n",
    "    ) -> None:\n",
    "        # (h_dimesnion, 1) @ \n",
    "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.U -= learning_rate * self.delta_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a13b6992-16d2-44cd-b2f6-3359603c6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    states: np.ndarray = None # Stores activations of all time steps\n",
    "    W: np.ndarray = None # Recurrent weight matrix\n",
    "    delta_W: np.ndarray = None # gradient of W during BPTT\n",
    "    bias: np.ndarray = None # b in the math formulas\n",
    "    delta_bias: np.ndarray = None # gradient of b\n",
    "    next_delta_activation: np.ndarray = None # stores the derivative of next step's loss function w.r.t. current activation\n",
    "\n",
    "    def __init__(self, vocab_size: int, size: int) -> None:\n",
    "        self.W = np.random.uniform(low=0, high=1, size=(size, size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.states = np.zeros(shape=(vocab_size, size, 1))\n",
    "        self.next_delta_activation = np.zeros(shape=(size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "\n",
    "    ## Return the hidden state value at a given time step. if time step is leass than 0\n",
    "    ## default to all zeros matrix\n",
    "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
    "        # If starting out at the beginning of the sequence, a[t-1] will return zero\n",
    "        if time_step < 0:\n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "\n",
    "    # Updating the state at a time step after forward pass calculation\n",
    "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray) -> None:\n",
    "        self.states[time_step] = hidden_state\n",
    "\n",
    "    # Forward pass calculation\n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
    "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_hidden_state = self.W @ previous_hidden_state\n",
    "        # (h_dimension, 1) + (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
    "        activation = np.tanh(weighted_sum) # (h_dimension, 1)\n",
    "        self.set_hidden_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    # Gradients of w and b\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_output: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        # (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        delta_activation = delta_output + self.next_delta_activation\n",
    "        # (h_dimension, 1) * scalar = (h_dimension, 1)\n",
    "        delta_weighted_sum = delta_activation * (\n",
    "            1 - self.get_hidden_state(time_step) ** 2\n",
    "        )\n",
    "        # (h_dimension, h_dimension) @ (1, h_dimension) = (h_dimension, h_dimension)\n",
    "        self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step - 1).T\n",
    "\n",
    "        # derivative of hidden bias is the same as dL_ds\n",
    "        self.delta_bias += delta_weighted_sum\n",
    "        return delta_weighted_sum\n",
    "\n",
    "    # updating the parameters of gradients\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.W -= learning_rate * self.delta_W\n",
    "        self.bias -= learning_rate * self.delta_bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e99b50d4-4a02-4306-881c-a9a3286370ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output layer\n",
    "class OutputLayer:\n",
    "    states: np.ndarray = None # internal memory of the networkk\n",
    "    V: np.ndarray = None # output weight matrix\n",
    "    bias: np.ndarray = None # c in the math formulas \n",
    "    delta_bias: np.ndarray = None # gradient of c \n",
    "    delta_V: np.ndarray = None # gradient of V during BPTT\n",
    "\n",
    "    def __init__(self, size: int, hidden_size: int) -> None:\n",
    "        self.V = np.random.uniform(low=0, high=1, size=(size, hidden_size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.states = np.zeros(shape=(size, size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "\n",
    "    #  forward pass to calculate the weighted output and probability distribution with softmax\n",
    "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        # V @ h => (input_size, h_dimension) @ (h_dimension, 1) = (input_size, 1)\n",
    "        # (input_size, 1) + (input_size, 1) = (input_size, 1)\n",
    "        output = self.V @ hidden_state + self.bias\n",
    "        prediction = softmax(output)\n",
    "        self.set_state(time_step, prediction)\n",
    "        return prediction\n",
    "\n",
    "    # return the output state (prediction) value at a given time step\n",
    "    def get_state(self, time_step: int) -> np.ndarray:\n",
    "        return self.states[time_step]\n",
    "\n",
    "    #  updating the output state at a time step after forward pass calculation.\n",
    "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
    "        self.states[time_step] = prediction\n",
    "\n",
    "    # compute gradients of V and c\n",
    "    def calculate_deltas_per_step(\n",
    "        self, \n",
    "        expected: np.ndarray, \n",
    "        hidden_state: np.ndarray, \n",
    "        time_step: int\n",
    "    ) -> np.ndarray:\n",
    "        # dL_do = dL_dyhat * dyhat_do = derivative of loss function * derivative of softmax\n",
    "        # dL_do = step.y_hat - expected[step_number]\n",
    "        delta_output = self.get_state(time_step) - expected # (input_size, 1)\n",
    "\n",
    "        self.delta_V += delta_output @ hidden_state.T\n",
    "\n",
    "        self.delta_bias += delta_output\n",
    "        return self.V.T @ delta_output\n",
    "\n",
    "    # updating the parameters using the gradient\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.V -= learning_rate * self.delta_V\n",
    "        self.bias -= learning_rate * self.delta_bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8fb7a4-5264-42cf-b510-33abb6bdc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4338b61e-dcd2-46c5-a30f-5fb83d27318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    hidden_layer: HiddenLayer\n",
    "    output_layer: OutputLayer\n",
    "    alpha: float  # learning rate\n",
    "    input_layer: InputLayer = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, alpha: float) -> None:\n",
    "        self.hidden_layer = HiddenLayer(vocab_size, hidden_size)\n",
    "        self.output_layer = OutputLayer(vocab_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
    "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
    "        for step in range(len(inputs)):\n",
    "            weighted_input = self.input_layer.weighted_sum(step)\n",
    "            activation = self.hidden_layer.activate(weighted_input, step)\n",
    "            self.output_layer.predict(activation, step)\n",
    "        return self.output_layer\n",
    "\n",
    "    def backpropagation(self, expected: np.ndarray) -> None:\n",
    "        for step_number in reversed(range(len(expected))):\n",
    "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
    "                expected[step_number],\n",
    "                self.hidden_layer.get_hidden_state(step_number),\n",
    "                step_number,\n",
    "            )\n",
    "            delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(\n",
    "                step_number, delta_output\n",
    "            )\n",
    "            self.input_layer.calculate_deltas_per_step(step_number, delta_weighted_sum)\n",
    "\n",
    "        self.output_layer.update_weights_and_bias(self.alpha)\n",
    "        self.hidden_layer.update_weights_and_bias(self.alpha)\n",
    "        self.input_layer.update_weights_and_bias(self.alpha)\n",
    "\n",
    "    def loss(self, y_hat: List[np.ndarray], y: List[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Cross-entropy loss function - Calculating difference between 2 probability distributions.\n",
    "        First, calculate cross-entropy loss for each time step with np.sum, which returns a numpy array\n",
    "        Then, sum across individual losses of all time steps with sum() to get a scalar value.\n",
    "        :param y_hat: predicted value\n",
    "        :param y: expected value - true label\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n",
    "\n",
    "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"epoch={epoch}\")\n",
    "            for idx, input in enumerate(inputs):\n",
    "                y_hats = self.feed_forward(input)\n",
    "                self.backpropagation(expected[idx])\n",
    "                print(f'Loss round: {self.loss([y for y in y_hats.states], expected[idx])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f399401e-e718-4a2b-8f39-49bf22369606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "Loss round: [258.07744889]\n",
      "Loss round: [251.003623]\n",
      "Loss round: [237.64500754]\n",
      "Loss round: [216.04396964]\n",
      "Loss round: [198.61126984]\n",
      "epoch=1\n",
      "Loss round: [185.05408263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9051/2388900243.py:46: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss round: [176.52511383]\n",
      "Loss round: [166.3217971]\n",
      "Loss round: [156.79101613]\n",
      "Loss round: [152.19618862]\n",
      "epoch=2\n",
      "Loss round: [148.09753291]\n",
      "Loss round: [144.20883191]\n",
      "Loss round: [139.95248213]\n",
      "Loss round: [135.80280863]\n",
      "Loss round: [134.07294609]\n",
      "epoch=3\n",
      "Loss round: [133.16657197]\n",
      "Loss round: [132.74015163]\n",
      "Loss round: [133.17719793]\n",
      "Loss round: [131.79141193]\n",
      "Loss round: [131.80301695]\n",
      "epoch=4\n",
      "Loss round: [132.55487368]\n",
      "Loss round: [133.72217959]\n",
      "Loss round: [133.68342362]\n",
      "Loss round: [132.91677432]\n",
      "Loss round: [133.91231748]\n",
      "epoch=5\n",
      "Loss round: [135.59427731]\n",
      "Loss round: [137.84004223]\n",
      "Loss round: [136.36046176]\n",
      "Loss round: [138.67763905]\n",
      "Loss round: [141.45929627]\n",
      "epoch=6\n",
      "Loss round: [144.1266439]\n",
      "Loss round: [147.94379505]\n",
      "Loss round: [149.83785316]\n",
      "Loss round: [153.08789014]\n",
      "Loss round: [157.14484135]\n",
      "epoch=7\n",
      "Loss round: [160.53942399]\n",
      "Loss round: [165.93550777]\n",
      "Loss round: [168.97673589]\n",
      "Loss round: [173.84681062]\n",
      "Loss round: [179.77954496]\n",
      "epoch=8\n",
      "Loss round: [183.08293392]\n",
      "Loss round: [189.23948743]\n",
      "Loss round: [192.32634934]\n",
      "Loss round: [199.04867282]\n",
      "Loss round: [203.51099008]\n",
      "epoch=9\n",
      "Loss round: [206.60039634]\n",
      "Loss round: [213.23657138]\n",
      "Loss round: [214.95690621]\n",
      "Loss round: [219.95441751]\n",
      "Loss round: [221.75324318]\n",
      "19\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  inputs = np.array([\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "      [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "      [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "      [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "  ])\n",
    "\n",
    "  expected = np.array([\n",
    "      [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "      [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "      [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "  ])\n",
    "  \n",
    "  one_hot_inputs = string_to_one_hot(inputs)\n",
    "  one_hot_expected = string_to_one_hot(expected)\n",
    "\n",
    "  # Forward pass through time, no gradient clipping yet so there will be gradient exploding problem\n",
    "  # https://stackoverflow.com/a/33980220\n",
    "  # https://stackoverflow.com/a/72494516\n",
    "  rnn = VanillaRNN(vocab_size=len(string.ascii_uppercase), hidden_size=128, alpha=0.0001)\n",
    "  rnn.train(one_hot_inputs, one_hot_expected, epochs=10)\n",
    "\n",
    "  new_inputs = np.array([[\"B\", \"C\", \"D\"]])\n",
    "  for input in string_to_one_hot(new_inputs):\n",
    "      predictions = rnn.feed_forward(input)\n",
    "      output = np.argmax(predictions.states[-1])\n",
    "      print(output) # index of the one-hot value of prediction\n",
    "      print(string.ascii_uppercase[output]) # mapping one hot to character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2fa76-67e4-46a5-a9b9-28d2f52a1fb5",
   "metadata": {},
   "source": [
    "## PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22fe240-2157-4e22-8d2b-d6d7a97b29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.2638\n",
      "Epoch [2/10], Loss: 3.2507\n",
      "Epoch [3/10], Loss: 3.2377\n",
      "Epoch [4/10], Loss: 3.2247\n",
      "Epoch [5/10], Loss: 3.2116\n",
      "Epoch [6/10], Loss: 3.1983\n",
      "Epoch [7/10], Loss: 3.1847\n",
      "Epoch [8/10], Loss: 3.1707\n",
      "Epoch [9/10], Loss: 3.1561\n",
      "Epoch [10/10], Loss: 3.1409\n",
      "17\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inputs = np.array([\n",
    "        [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "        [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "        [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "        [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "        [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "    ])\n",
    "    expected = np.array([\n",
    "        [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "        [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "        [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"],\n",
    "        [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "        [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "    ])\n",
    "    \n",
    "    # Encode strings to int indexes\n",
    "    input_encoded = np.vectorize(string.ascii_uppercase.index)(inputs)\n",
    "    expected_encoded = np.vectorize(string.ascii_uppercase.index)(expected)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    # PyTorch RNN expects input shape as (seq_len, batch, input_size)\n",
    "    # We'll transpose the data to match this format\n",
    "    X = torch.FloatTensor(input_encoded).long()\n",
    "    y = torch.FloatTensor(expected_encoded).long()\n",
    "    \n",
    "    # One-hot encode inputs and targets\n",
    "    # PyTorch has a cleaner way to handle this with embedding or using CrossEntropyLoss\n",
    "    X_one_hot = torch.nn.functional.one_hot(X, num_classes=26).float()\n",
    "    y_one_hot = torch.nn.functional.one_hot(y, num_classes=26).float()\n",
    "    \n",
    "    # Reshape to (batch_size, seq_len, features)\n",
    "    X_one_hot = X_one_hot\n",
    "    y_one_hot = y_one_hot\n",
    "    \n",
    "    # Create PyTorch model\n",
    "    class SimpleRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(SimpleRNN, self).__init__()\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, seq_len, input_size)\n",
    "            rnn_out, _ = self.rnn(x)\n",
    "            # rnn_out shape: (batch_size, seq_len, hidden_size)\n",
    "            output = self.fc(rnn_out)\n",
    "            # output shape: (batch_size, seq_len, output_size)\n",
    "            return output\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = SimpleRNN(input_size=26, hidden_size=128, output_size=26)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_one_hot)\n",
    "        \n",
    "        # Compute loss\n",
    "        # Reshape for CrossEntropyLoss: [batch_size*seq_len, num_classes]\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, 26), \n",
    "            y.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Test with new input\n",
    "    new_inputs = np.array([[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"]])\n",
    "    new_inputs_encoded = np.vectorize(string.ascii_uppercase.index)(new_inputs)\n",
    "    new_inputs_tensor = torch.tensor(new_inputs_encoded).long()\n",
    "    new_inputs_one_hot = torch.nn.functional.one_hot(new_inputs_tensor, num_classes=26).float()\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(new_inputs_one_hot)\n",
    "        \n",
    "    # Get prediction of last time step and last element\n",
    "    prediction = torch.argmax(prediction[0, -1]).item()\n",
    "    \n",
    "    print(prediction)\n",
    "    print(string.ascii_uppercase[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4e598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
